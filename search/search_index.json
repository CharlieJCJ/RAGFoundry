{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A framework for enhancing LLMs for RAG use-cases by enabling users to create data-augmented datasets for tuning and evaluation of LLMs, using RAG workflows.</p> <p>RAG Foundry is a library designed to improve LLMs ability to use external information by fine-tuning models on specially created RAG-augmented datasets. The library helps create the data for training, given a RAG technique, helps easily train models using parameter-efficient finetuning (PEFT), and finally can help users measure the improved performance using various, RAG-specific metrics. The library is modular, workflows are customizable using configuration files.</p>"},{"location":"#installation","title":"Installation","text":"<p>Clone locally and run:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"#overview","title":"Overview","text":"<p>The RAG Foundry framework facilitates fast prototyping and experimentation with various RAG settings and configurations, including data selection and filtering, processing, retrieval, ranking, query manipulation, prompt generation, training, inference, output processing and evaluation. The library is comprised of 4 modules: dataset creation, training, inference and evaluation.</p> <ul> <li> <p>Dataset Creation: The processing module creates datasets, persisting RAG interactions, to be used for RAG training and inference. RAG interactions include dataset loading, columns normalization, data aggregation (fewshot creation), information retrieval using external tools and frameworks, API integration, template-based prompt creation and any other form of pre-processing. The data is saved in a consistent, model-independent, input-output format, along with all other fields and metadata. See Processing.md.</p> </li> <li> <p>Training: using PEFT for efficient training and TRL (e.g. supervised FT) users can train any model on the augmented datasets. Training is done on the completions. Models can be pushed to HF Hub. See Training.md.</p> </li> <li> <p>Inference: generating predictions using the augmented datasets with trained or untrained LLMs. See Inference.md.</p> </li> <li> <p>Evaluation: running evaluation on the generated output from the inference module. Users can provide a list of metrics to run; custom metrics can be implemented easily. Current metrics include EM, F1, ROUGE, BERTScore, Deepeval, RAGAS, HF <code>evaluate</code> and classification. Metrics can be local\u2014run on each example, or global\u2014run on the entire dataset, e.g. recall. Metrics can utilize any feature in the dataset, like retrieval results, reasoning, citations and attributions, not just the input and output texts. See Evaluation.md.</p> </li> </ul>"},{"location":"#running","title":"Running","text":"<p>The 4 modules are represented as scripts: <code>processing.py</code>, <code>training.py</code>, <code>inference.py</code> and <code>evaluation.py</code> at the top level. Every call has the form <code>python SCRIPT options...</code>.</p> <p>The library utilizes the Hydra configuration tool; it enables the use of hierarchical configurations, easily overridden of values in the CLI and the ability to run multiple jobs remotely (e.g. integrations with SLURM and Ray). It represents a configuration-as-code approach, as it can instantiate python classes according to configuration (the <code>_target_</code> keyword indicates the python class to use in a given context).</p> <p>There are default configurations for each module in the configs folder. A configuration file can be overridden like so:</p> <pre><code>python processing -cp configs/paper -cn processing-asqa-retrieval\n</code></pre> <p>Individual keywords can be overridden as well: <pre><code>python processing -cp configs/paper -cn processing-asqa-retrieval   \\\n       output_path=/store/data/here                                 \\\n       hfhub_tag=my_org/my_data\n</code></pre></p> <p>For a complete set of configurations, reproducing the experimentation in the paper with the ASQA dataset, see the configurations in the Paper folder.</p>"},{"location":"#license","title":"License","text":"<p>The code is licensed under the Apache 2.0 License.</p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>This is not an official Intel product.</p>"},{"location":"evaluation/","title":"Evaluations","text":"<p>The evaluation module takes the produced inference file and the original processed dataset and runs a list of evaluations, producing a final results file, in a YAML format. The evaluations are represented as metric classes.</p> <p>We implement several metrics including: a wrapper for HuggingFace <code>evaluate</code> class, which can accept a list of metrics, EM, F1, classification (accuracy, precision, recall, F1), BERTScore, Semantic similarity (using a customizable cross-encoder). The module can also run metrics from DeepEval, which offers a large collection of LLM evaluations.</p> <p>Metrics can be either local or global; a local metric runs over each example individually, scores are collected and averaged. A global metric runs on the entire dataset at once, for example: classification F1.</p> <p>The configuration contains the following section:</p> <p><pre><code>answer_processor:\n  _target_: ragfoundry.processing.answer_processors.regex.RegexAnswer\n  capture_pattern:          # \"&lt;ANSWER&gt;: (.*)\"\n  stopping_pattern:         # \"[,.;]\"\n</code></pre> The evaluation module introduces the concept of an Answer Processor. This class can run post-processing on the generated text, preparing it for evaluations or the specific format some metrics require.</p> <p>There is a default processor, called <code>RegexAnswer</code>; it can filter text, based on a python regex capture pattern. It can also split text using a stopping pattern. For example, in the Chain-of-Thought reasoning we used in the paper, the model is instruction to explain its answer, cite if needed and finally print the final results in the following format <code>&lt;ANSWER&gt;: ...</code>. We can use this format as a capture pattern; thus models that learn to answer using this pattern (obey the instruction) will score higher.</p> <p>Next is a list of metrics; each one is a python class: <pre><code>metrics:\n  - _target_: ragfoundry.evaluation.metrics.HFEvaluate\n    metric_names: [rouge]\n  - _target_: ragfoundry.evaluation.metrics.EM\n  - _target_: ragfoundry.evaluation.metrics.F1\n  - _target_: ragfoundry.evaluation.metrics.BERTScore\n    model: microsoft/deberta-large-mnli\n</code></pre></p> <p>Some metrics require additional parameters, for example HuggingFace <code>evaluate</code> requires the metrics' names, BERTScore requires an embedding model.</p> <p><pre><code>key_names:\n  generated: generated\n  label: answer\n  query: query\n  context: context\n</code></pre> A mapping of keys and values: the values should represent the names of the corresponding fields in the processed dataset.</p> <p>Finally: <pre><code>results_file: my-evaluation.yaml\ngenerated_file: inference.jsonl\ndata_file: my-processed-data.jsonl\nlimit:\n</code></pre></p> <p>One needs to provide the generated inference file, the processed dataset and a filename for the results summary. A limit number of rows can be provided for debugging purposes.</p>"},{"location":"evaluation/#running-evaluations-on-asqa","title":"Running Evaluations on ASQA","text":"<p>As the final part of the demonstration of the framework with the ASQA dataset and Phi-3 models, we will evaluate the different RAG configurations, with and without the use of fine-tuning.</p> <p>As a reminder, ASQA has 2 types of answers: long answer and short answers. We will evaluate the generated answers using the long answer with RAGAS metrics (faithfulness and relevancy) and use the short answers with ASQA defined STR-EM.</p>"},{"location":"evaluation/#short","title":"Short","text":"<p>Starting with the short answers, the label keyword is <code>answer-short</code> (recall the processing) and a representative configuration looks like this:</p> <pre><code>answer_processor:\n    _target_: ragfoundry.processing.answer_processors.regex.RegexAnswer\n    capture_pattern: \"&lt;ANSWER&gt;: (.*)\"\n    stopping_pattern:\n\nmetrics:\n    - _target_: ragfoundry.evaluation.metrics.StringEM\n\nkey_names:\n    generated: text\n    label: answer-short\n    query: query\n\nresults_file: evaluation-asqa-baseline.yaml\ngenerated_file: asqa-baseline-dev-generated.jsonl\ndata_file: asqa-baseline-dev.jsonl\n</code></pre> <p>Here are the calls to evaluate the different configurations:</p> <p>Baseline: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-short\n</code></pre></p> <p>Context: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-short    \\\n       results_file=asqa-context-dev-generated-results.yaml         \\\n       data_file=asqa-context-dev.jsonl                             \\\n       generated_file=asqa-context-dev-generated.jsonl\n</code></pre></p> <p>Context with fine-tuned model: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-short    \\\n       results_file=asqa-context-ft-dev-generated-results.yaml      \\\n       data_file=asqa-context-dev.jsonl                             \\\n       generated_file=asqa-context-ft-dev-generated.jsonl\n</code></pre></p> <p>Chain-of-Thought: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-short    \\\n       results_file=asqa-cot-dev-generated-results.yaml             \\\n       data_file=asqa-cot-dev.jsonl                                 \\\n       generated_file=asqa-cot-dev-generated.jsonl\n</code></pre></p> <p>Chain-of-Thought with fine-tuned model: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-short    \\\n       results_file=asqa-cot-ft-dev-generated-results.yaml          \\\n       data_file=asqa-cot-dev.jsonl                                 \\\n       generated_file=asqa-cot-ft-dev-generated.jsonl\n</code></pre></p>"},{"location":"evaluation/#long","title":"Long","text":"<p>Evaluation the generated output with respect to the full answer, we use two RAGAS metrics, namely faithfulness and relevancy. The RAGAS metrics require a context for the critic to make a judgment, so these are not relevant for the baseline configuration.</p> <p>The different in configuration is in the list of metrics and keywords:</p> <pre><code>metrics:\n    - _target_: ragfoundry.evaluation.deep.Faithfulness\n      azure_endpoint: azure.endpoint.com\n      azure_deployment: GPT-4-32k-Bot\n      api_version: 2024-05-01-preview\n    - _target_: ragfoundry.evaluation.deep.Relevancy\n      azure_endpoint: azure.endpoint.com\n      azure_deployment: GPT-4-32k-Bot\n      api_version: 2024-05-01-preview\n      embeddings: BAAI/bge-small-en-v1.5\n\nkey_names:\n    generated: text\n    label: answers\n    query: query\n    context: positive_passages\n</code></pre> <p>The relevancy metrics an embedder\u2014it generates probable questions based on the generated answer (and the context) and then measures semantic similarity to the original question.</p> <p>Context: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-long     \\\n       results_file=asqa-context-dev-generated-results-ragas.yaml   \\\n       data_file=asqa-context-dev.jsonl                             \\\n       generated_file=asqa-context-dev-generated.jsonl\n</code></pre></p> <p>Context with fine-tuned model: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-long         \\\n       results_file=asqa-context-ft-dev-generated-results-ragas.yaml    \\\n       data_file=asqa-context-dev.jsonl                                 \\\n       generated_file=asqa-context-ft-dev-generated.jsonl\n</code></pre></p> <p>Chain-of-Thought: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-long \\\n       results_file=asqa-cot-dev-generated-results-ragas.yaml   \\\n       data_file=asqa-cot-dev.jsonl                             \\\n       generated_file=asqa-cot-dev-generated.jsonl\n</code></pre></p> <p>Chain-of-Thought with fine-tuned model: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-long     \\\n       results_file=asqa-cot-ft-dev-generated-results-ragas.yaml    \\\n       data_file=asqa-cot-dev.jsonl                                 \\\n       generated_file=asqa-cot-ft-dev-generated.jsonl\n</code></pre></p>"},{"location":"inference/","title":"Inference","text":"<p>In the inference stage, we take the processed dataset and LLM and make predictions. The LLM can be fine-tuned. The processed data encapsulates the RAG interactions: pre-processing, retrieval, ranking, prompt-creation, and possibly other types of transformations. So this step deals with producing the predictions to be evaluated.</p> <p>It is simple in nature, described by the following configuration:</p> <pre><code>model:\n    _target_: ragfoundry.models.hf.HFInference\n    model_name_or_path: microsoft/Phi-3-mini-128k-instruct\n    load_in_4bit: false\n    load_in_8bit: true\n    device_map: auto\n    torch_dtype:\n    trust_remote_code: true\n    instruction: ragfoundry/processing/prompts/prompt_instructions/qa.txt\n    instruct_in_prompt: false\n    lora_path:\n    generation:\n        do_sample: false\n        max_new_tokens: 50\n        max_length:\n        temperature:\n        top_k:\n        top_p:\n        return_full_text: false\n\ndata_file: asqa-baseline-dev.jsonl\ngenerated_file: asqa-baseline-dev-generated.jsonl\ninput_key: prompt\ngeneration_key: output\ntarget_key: answers\nlimit:\n</code></pre> <p>The model section deals with details regarding the model loading and generation options. System instruction can be provided, as we mentioned previously: the datasets are model independent, and all model details (system instruction, custom chat template) are needed only during training and inference. Similarly, <code>instruct_in_prompt</code> inserts the system instruction inside the prompt, for models which don't support a system role.</p> <p>Other parameters: - Data file is the processed file. - Generated file is the file that will be created with the completions (and labels, for easy debugging). - Target key is the label keyword. - Limit: to a number of examples, for debugging.</p>"},{"location":"inference/#running-inference","title":"Running Inference","text":"<p>In order to run evaluations for ASQA, like in the paper, there are 5 configurations to run: baseline, context, context with fine-tuned model, CoT reasoning, and CoT reasoning with a model that was fine-tuned with distractor documents.</p> <p>The baseline inference uses the configuration as is; the other calls, use the configuration and just override the value of the processed data to use and optionally LORA path for the model.</p> <p>Baseline: <pre><code>python inference.py -cp configs/paper -cn inference-asqa\n</code></pre></p> <p>Context: <pre><code>python inference.py -cp configs/paper -cn inference-asqa    \\\n       data_file=asqa-context-dev.jsonl                     \\\n       generated_file=asqa-context-dev-generated.jsonl\n</code></pre></p> <p>Context with fine-tuned model: <pre><code>python inference.py -cp configs/paper -cn inference-asqa    \\\n       data_file=asqa-context-dev.jsonl                     \\\n       generated_file=asqa-context-ft-dev-generated.jsonl   \\\n       model.lora_path=./path/to/lora/checkpoint\n</code></pre></p> <p>Chain-of-Thought: <pre><code>python inference.py -cp configs/paper -cn inference-asqa    \\\n       data_file=asqa-cot-dev.jsonl                         \\\n       generated_file=asqa-cot-ft-dev-generated.jsonl\n</code></pre></p> <p>Chain-of-Thought with fine-tuned model: <pre><code>python inference.py -cp configs/paper -cn inference-asqa    \\\n       data_file=asqa-cot-dev.jsonl                         \\\n       generated_file=asqa-cot-ft-dev-generated.jsonl       \\\n       model.lora_path=./path/to/lora/checkpoint\n</code></pre></p>"},{"location":"processing/","title":"Data Augmentation","text":"<p>To demonstrate the usage of RAG Foundry data augmentation, we will follow the experimentation presented in the paper. Choosing the ASQA Q&amp;A dataset and the Phi-3 model. We compare a baseline configuration with 4 other configurations:</p> <ol> <li>Retrieval augmentation using a corpus and inserting the documents in the prompt after the question.</li> <li>Similar to (1) but having the model fine-tune on the completions.</li> <li>Similar to (1) and adding a Chain-of-Thought instruction for the model to explain its reasoning and format its answer.</li> <li>Similar to (3) but having the model fine-tune on the completions while implementing a technique from RAFT where distracting documents are used.</li> </ol> <p>The ASQA dataset has two types of answer: a long answer and lists of short answers (actually list of lists). Additionally, it has some minimal amount of context in the data, so we augment it using a corpus, stored as a vector DB; we use Qdrant.</p> <p>In order to train configuration (4), we need to have CoT well-reasoned responses as labels, so we use OpenAI GPT4 model to augment a dataset with these synthetic labels.</p> <p>Notice: all the configurations mentioned here, implementing the experiments done in the paper, are saved in <code>configs/paper/</code>. They don't run by default, they need to be specified by running:</p> <pre><code>python module-name.py -cp configs/paper -cn config-name-without-extension\n</code></pre>"},{"location":"processing/#retrieval","title":"Retrieval","text":"<p>The first step would be to augment the entire dataset (train, dev) with relevant documents, based on the questions, see processing-asqa-retrieval.yaml. Let's focus on the different steps:</p> <pre><code>- _target_: ragfoundry.processing.dataset_loaders.loaders.HFLoader\n  inputs: train\n  dataset_config:\n        path: din0s/asqa\n        split: train\n\n- _target_: ragfoundry.processing.dataset_loaders.loaders.HFLoader\n  inputs: dev\n  dataset_config:\n        path: din0s/asqa\n        split: dev\n</code></pre> <p>We load the train and dev splits, to be used in the pipeline; they will be referred using the <code>inputs</code> keyword used in this step.</p> <p><pre><code>- _target_: ragfoundry.processing.local_steps.common_datasets.ASQA\n  inputs: [train, dev]\n</code></pre> We do some minimal processing, related to ASQA, namely column renaming, collecting the short and long answers and having a consistent scheme, for example: <code>query</code>, <code>answers</code>, <code>positive_passages</code>, etc. Feel free to add your own types of pre-processing.</p> <p>Notice the <code>inputs</code> keyword can accept a list of strings, meaning the step will run over the datasets specified.</p> <p><pre><code>- _target_:\n        ragfoundry.processing.local_steps.retrievers.haystack.HaystackRetriever\n  inputs: [train, dev]\n  pipeline_or_yaml_path: ./configs/external/haystack/qdrant.yaml\n  docs_key: positive_passages\n  query_key: query\n</code></pre> This is the retrieval step. We use the Haystack framework for building RAG pipelines; in this example, the Haystack pipeline is comprised of an embedder and a retriever, connecting the Qdrant using a Qdrant-Haystack integration (all defined in the requirements file). The Haystack pipeline is initialized from the Qdrant.yaml configuration. One can use other frameworks for retrieval, like LangChain, LlamaIndex, or others.</p> <p>The retrieval step will store the most relevant documents (k=5) in the <code>docs_key</code> and the query will be defined by the <code>query_key</code>.</p> <p><pre><code>- _target_: ragfoundry.processing.local_steps.context.ContextHandler\n  inputs: [train, dev]\n  docs_key: positive_passages\n</code></pre> In this simple step, the documents retrieved are processed; they have a title and content fields and this step combine these into a single string for every document. This step may be unnecessary, depending on the retrieval mechanism and format.</p> <p><pre><code>- _target_: ragfoundry.processing.global_steps.sampling.Sampler\n  inputs: [train, dev]\n  k: 1\n  input_key: positive_passages\n  output_key: negative_passages\n</code></pre> The <code>Sampler</code> class deals with sampling examples from the same dataset or others. In order to train the RAFT-based model on a combination of relevant and distracting documents, we need to collect these distracting documents. Here we chose to collect positive documents from other examples, to be used as negative documents. The <code>Sampler</code> is then ran with k=1, it collects only the <code>positive_passages</code> from the examples it samples and store them in a new keyword, called <code>negative_passages</code>.</p> <p><pre><code>- _target_: ragfoundry.processing.global_steps.output.OutputData\n  inputs: [train, dev]\n  prefix: asqa\n</code></pre> Finally we write the two resulting dataset to disk. They represent the retrieval-augmented datasets, ready to be processed for the different tasks.</p> <p>To run this process: <pre><code>python processing.py -cp configs/paper -cn processing-asqa-retrieval\n</code></pre></p>"},{"location":"processing/#baseline-configuration","title":"Baseline Configuration","text":"<p>For the baseline, there is not going to be context, only the question presented to the model. We use instruction-following models that have a chat template built-in. The framework populates the chat template using the inputs and outputs we generate, so we don't need to worry about roles and special tokens. Additionally, the system instruction is specified only during training and inference: it needn't be part of the dataset so these next steps mainly deal with the prompt generation.</p> <p>These are the interesting steps:</p> <pre><code>- _target_: ragfoundry.processing.dataset_loaders.loaders.LocalLoader\n  inputs: dev\n  filename: asqa-dev.jsonl\n\n- _target_: ragfoundry.processing.local_steps.prompter.TextPrompter\n  inputs: dev\n  prompt_file: ragfoundry/processing/prompts/qa-short.txt\n  output_key: prompt\n  mapping:\n        query: query\n</code></pre> <p>We load the locally retrieval-augmented files we generated in the previous section.</p> <p>The <code>TextPrompter</code> populates a template file containing placeholders in python format, see the short template. The step replace the placeholders with variables using a provided mapping. The result is a string, saved in a keyword called <code>outputs_key</code>.</p> <p>To run this process: <pre><code>python processing.py -cp configs/paper -cn processing-asqa-baseline\n</code></pre></p>"},{"location":"processing/#context","title":"Context","text":"<p>Preparing for configurations (1) and (2), we want to augment the examples with the top 5 documents we collected in the first step.</p> <p><pre><code>- _target_: ragfoundry.processing.local_steps.context.DocumentsJoiner\n  inputs: [train, dev]\n  docs_key: positive_passages\n  k: 5\n\n- _target_: ragfoundry.processing.local_steps.prompter.TextPrompter\n  inputs: [train, dev]\n  prompt_file: ragfoundry/processing/prompts/qa.txt\n  output_key: prompt\n  mapping:\n        question: query\n        context: positive_passages\n</code></pre> The <code>DocumentJoiner</code> joins a list of strings and is needed before the <code>TextPrompter</code> we've seen from the previous section. We prepare a dev file\u2014for testing the model with retrieved documents\u2014and also a training file, in order to run fine-tuning. Both configurations will be evaluated on the dev dataset.</p> <p>To run this process: <pre><code>python processing.py -cp configs/paper -cn processing-asqa-context\n</code></pre></p>"},{"location":"processing/#chain-of-thought","title":"Chain-of-Thought","text":"<p>We prepare a dev set with CoT reasoning prompt. The configuration will be similar to the Context configuration, however here we use a different prompt template:</p> <pre><code>- _target_: ragfoundry.processing.local_steps.prompter.TextPrompter\n  inputs: dev\n  prompt_file: ragfoundry/processing/prompts/cot.txt\n  output_key: prompt\n  mapping:\n        question: query\n        context: positive_passages\n</code></pre> <p>To run this process: <pre><code>python processing.py -cp configs/paper -cn processing-asqa-cot-dev\n</code></pre></p>"},{"location":"processing/#chain-of-thought-training-dataset","title":"Chain-of-Thought Training Dataset","text":"<p>In order to train a model on a CoT-based prompt, we need to collect well-reasoned responses; we use GPT4 for that. Additionally, we implement a technique from RAFT where some percentage of the examples have purely distractor documents, in order for the model ability to filter noise. Here are the relevant steps:</p> <p><pre><code>- _target_: ragfoundry.processing.local_steps.raft.RAFTStep\n  inputs: train\n  k: 5\n  raft_p: 0.5\n  neg_docs_num: 2\n  output_key: raft_docs\n</code></pre> The <code>RAFTStep</code> implements the logic presented in the paper; the percentage of purely-distractor documents is defined by <code>raft_p</code>. The list of documents, some relevant, some distracting, are saved in a keyword called <code>output_key</code>.</p> <p><pre><code>- _target_: ragfoundry.processing.local_steps.context.DocumentsJoiner\n  inputs: train\n  docs_key: raft_docs\n  k:\n\n- _target_: ragfoundry.processing.local_steps.prompter.TextPrompter\n  inputs: train\n  prompt_file: ragfoundry/processing/prompts/cot.txt\n  output_key: prompt\n  mapping:\n        question: query\n        context: raft_docs\n</code></pre> The documents are joined into strings; when <code>k:</code> all documents are used. The prompt used is the same as when building the dev dataset.</p> <p>Next is interacting with OpeanAI; we implemented an OpenAI class using Azure, one can implement using other abstractions. The step itself needs the <code>prompt_key</code>, instruction file and the results are saved in the <code>answer_key</code>. <pre><code>- _target_: ragfoundry.processing.local_steps.api.openai.OpenAIChat\n  inputs: train\n  prompt_key: prompt\n  answer_key: generated_answer\n  instruction: ragfoundry/processing/prompts/prompt_instructions/qa.txt\n  model:\n        azure_endpoint: azure.endpoint.com\n        api_version: 2024-05-01-preview\n        model: GPT-4-32k-Bot\n</code></pre></p> <p>To run this process: <pre><code>python processing.py -cp configs/paper -cn processing-asqa-cot-train\n</code></pre></p>"},{"location":"training/","title":"Training","text":"<p>Training is done on the processed files. The training configuration has 3 parts: model, training arguments and data.</p> <p><pre><code>model:\n    _target_: ragfoundry.models.hf.HFTrain\n    model_name_or_path: microsoft/Phi-3-mini-128k-instruct\n    load_in_4bit: false\n    load_in_8bit: true\n    lora:\n        bias: none\n        fan_in_fan_out: false\n        lora_alpha: 16\n        lora_dropout: 0.1\n        peft_type: LORA\n        r: 16\n        target_modules:\n            - qkv_proj\n        task_type: CAUSAL_LM\n        use_rslora: true\n    completion_start: &lt;|assistant|&gt;\n    instruction_in_prompt:\n    max_sequence_len: 4000\n</code></pre> Model loading is done in the <code>HFTrain</code> class, which loads models from HuggingFace hub and uses PEFT adapters. Other classes can be implemented. The important keys here are: <code>completion_start</code> which indicates the beginning of the text where loss is to be calculated. This is model/tokenizer specific. Additionally, there is the <code>instruction_in_prompt</code> key, which if set to True, inserts the system instruction in the prompt, for models which do not support a dedicated system role.</p> <p>Next is the training arguments: <pre><code>train:\n    output_dir: ./trained_models/\n    bf16: false\n    fp16: false\n    gradient_accumulation_steps: 2\n    group_by_length:\n    learning_rate: 1e-4\n    logging_steps: 10\n    lr_scheduler_type: cosine\n    max_steps: -1\n    num_train_epochs: 1\n    per_device_train_batch_size: 1\n    optim: paged_adamw_8bit\n    remove_unused_columns: true\n    save_steps: 20000\n    save_total_limit: 1\n    warmup_ratio: 0.03\n    weight_decay: 0.001\n    report_to:\n</code></pre></p> <p>Training is done using the <code>SFTTrainer</code> in <code>TRL</code>. Training arguments are based on HuggingFace <code>Trainer</code>.</p> <p>Finally, data and other options: <pre><code>instruction: ragfoundry/processing/prompts/prompt_instructions/qa.txt\ntemplate:\ndata_file:\ninput_key: prompt\noutput_key:\nresume_checkpoint:\nlimit:\nshuffle:\nhfhub_tag:\nuse_wandb:\nexperiment:\nwandb_entity:\n</code></pre></p> <p>Here are they important keys:</p> <ul> <li>The instruction file to use for training (should later be used for inference as well).</li> <li>If the model/tokenizer do not support a chat template, the user needs to provided a custom template; they placeholders to fill are <code>query</code> and <code>output</code>.</li> <li>Data file is the processed file to train on.</li> <li>Input key is the prompt.</li> <li>Output key is completion text to learn.</li> <li>Limit and shuffle can be used to filter the dataset for debugging purposes.</li> <li>The framework can push the trained model to <code>hfhub_tab</code>.</li> <li>The last three keys related to experiment tracking using WANDB. Other services can be used by modifying the <code>report_to</code> key.</li> </ul>"},{"location":"training/#sending-runs","title":"Sending Runs","text":"<p>As we mentioned in the Data Augmentation page, we demonstrate the framework functionality using the ASQA dataset and the Phi-3 model, experimenting with 5 different configurations. Only 2 configurations require fine-tuning. One can send the training job like this:</p> <pre><code>python training.py -cp configs/paper -cn training-asqa  \\\n       data_file=asqa-prefix-train.jsonl                \\\n       output_key=answers                               \\\n       train.output_dir=./trained_models_context/\n</code></pre> <p>The <code>-cp</code> and <code>-cn</code> are overrides for the default configuration, which is <code>./configs/training.yaml</code>. Then there are overrides for the processed data file to use, the name of the label key and where to save the trained model. Overrides are based on the Hydra vocabulary.</p> <p>For the CoT model with RAFT contexts, we run: <pre><code>python training.py -cp configs/paper -cn training-asqa  \\\n       data_file=asqa-raft-cot-train.jsonl              \\\n       output_key=generated_answer                      \\\n       train.output_dir=./trained_models_cot/\n</code></pre></p>"},{"location":"blog/","title":"Blog","text":""},{"location":"reference/evaluation/base/","title":"Base","text":""},{"location":"reference/evaluation/base/#ragfoundry.evaluation.base.MetricBase","title":"<code>MetricBase</code>","text":"<p>Base class for metrics.</p> <p>Metrics can be local or global; local means score are calculated per example. Global means score is calculated by looking at the entire dataset, e.g. fluency.</p>"},{"location":"reference/evaluation/base/#ragfoundry.evaluation.base.MetricBase.measure","title":"<code>measure(example: dict) -&gt; dict[str, float]</code>","text":"<p>Measure the performance of the model on a given example.</p> <p>Parameters:</p> <ul> <li> <code>example</code>               (<code>dict</code>)           \u2013            <p>The example to evaluate the model on.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, float]</code>           \u2013            <p>dict[str, float]: A dictionary containing the performance metrics.</p> </li> </ul>"},{"location":"reference/evaluation/deep/","title":"DeepEval","text":""},{"location":"reference/evaluation/deep/#ragfoundry.evaluation.deep.DeepEvalBase","title":"<code>DeepEvalBase</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Base class for DeepEval metrics.</p> <p>Here we use AzureChatOpenAI interface; replace if needed.</p>"},{"location":"reference/evaluation/deep/#ragfoundry.evaluation.deep.Faithfulness","title":"<code>Faithfulness</code>","text":"<p>               Bases: <code>DeepEvalBase</code></p> <p>Faithfulness metric from DeepEval, based on RAGAS.</p> <p>Measures faithfulness of generated text by comparing it to the target text.</p>"},{"location":"reference/evaluation/deep/#ragfoundry.evaluation.deep.Hallucination","title":"<code>Hallucination</code>","text":"<p>               Bases: <code>DeepEvalBase</code></p> <p>Hallucination metric from DeepEval.</p> <p>Measures hallucination of generated text by comparing it to the retrieved documents.</p>"},{"location":"reference/evaluation/deep/#ragfoundry.evaluation.deep.Relevancy","title":"<code>Relevancy</code>","text":"<p>               Bases: <code>DeepEvalBase</code></p> <p>Answer relevancy metric from DeepEval, based on RAGAS.</p> <p>Measures relevancy of generated text by comparing it to the retrieved documents.</p>"},{"location":"reference/evaluation/metrics/","title":"Metrics","text":""},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.BERTScore","title":"<code>BERTScore</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>BERTScore metric, based on the BERTScore library.</p>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.BERTScore.__init__","title":"<code>__init__(key_names: dict, model='microsoft/deberta-large-mnli', **kwargs)</code>","text":"<p>Initialize the Metrics class.</p> <p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> <li> <code>model</code>               (<code>str</code>, default:                   <code>'microsoft/deberta-large-mnli'</code> )           \u2013            <p>The name of the BERT model to use. Defaults to \"microsoft/deberta-large-mnli\".</p> </li> </ul>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.Classification","title":"<code>Classification</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Metrics for classification answers: accuracy, precision, recall, F1; macro-averaged.</p> dict - mapping of labels to integers. <p>Example: {\"true\": 1, \"false\": 0, \"maybe\": 2}</p> <p>else_value: int - value to assign to labels not in the mapping.</p>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.EM","title":"<code>EM</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Implementing Exact Match based on code from Kilt.</p>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.EM.__init__","title":"<code>__init__(key_names, **kwargs) -&gt; None</code>","text":"<p>Initialize the Metrics class.</p> <p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> </ul>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.F1","title":"<code>F1</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Implementing F1 based on code from Kilt.</p>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.F1.__init__","title":"<code>__init__(key_names, **kwargs) -&gt; None</code>","text":"<p>Initialize the Metrics class.</p> <p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> </ul>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.HFEvaluate","title":"<code>HFEvaluate</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Wrapper class around <code>evaluate</code> metrics; easy to use, only need metric names.</p>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.HFEvaluate.__init__","title":"<code>__init__(key_names, metric_names: list[str], **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> <li> <code>metric_names</code>               (<code>list[str]</code>)           \u2013            <p>A list of metric names.</p> </li> </ul>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.HFEvaluate.measure","title":"<code>measure(example)</code>","text":"<p>Measure the performance of the model on a given example.</p> <p>Parameters:</p> <ul> <li> <code>example</code>               (<code>dict</code>)           \u2013            <p>The example containing input and target values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>The performance metric(s) computed for the example.</p> </li> </ul>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.Semantic","title":"<code>Semantic</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Semantic similarity between label and answer using a cross-encoder.</p>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.Semantic.__init__","title":"<code>__init__(key_names: dict, model: str = 'vectara/hallucination_evaluation_model', **kwargs) -&gt; None</code>","text":"<p>Initializes an instance of the class.</p> <p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> <li> <code>model</code>               (<code>str</code>, default:                   <code>'vectara/hallucination_evaluation_model'</code> )           \u2013            <p>The name of the BERT model to use.</p> </li> </ul>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.StringEM","title":"<code>StringEM</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Implementing String Exact Match.</p> <p>Used in ASQA to evaluate whether the annoated short answers appear in the generated answer as sub-strings.</p>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.StringEM.__init__","title":"<code>__init__(key_names: dict, **kwargs) -&gt; None</code>","text":"<p>Initialize the Metrics class.</p> <p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> </ul>"},{"location":"reference/evaluation/metrics/#ragfoundry.evaluation.metrics.normalize_text","title":"<code>normalize_text(s)</code>","text":"<p>Normalize the given text by lowercasing it, removing punctuation, articles, and extra whitespace.</p> <p>Parameters:</p> <ul> <li> <code>s</code>               (<code>str</code>)           \u2013            <p>The text to be normalized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>The normalized text.</p> </li> </ul>"},{"location":"reference/models/hf/","title":"Transformers","text":""},{"location":"reference/models/hf/#ragfoundry.models.hf.HFInference","title":"<code>HFInference</code>","text":"<p>Class for running HF model inference locally.</p>"},{"location":"reference/models/hf/#ragfoundry.models.hf.HFInference.__init__","title":"<code>__init__(model_name_or_path: str, torch_dtype, device_map, instruction: Path, instruct_in_prompt: False, template: Path = None, lora_path=None, generation=None, task='text-generation', **kwargs)</code>","text":"<p>Initialize a HF model, with optional LORA adapter.</p> <p>Parameters:</p> <ul> <li> <code>model_name_or_path</code>               (<code>str</code>)           \u2013            <p>HF model name or path.</p> </li> <li> <code>torch_dtype</code>               (<code>str</code>)           \u2013            <p>torch dtype for the model.</p> </li> <li> <code>device_map</code>           \u2013            <p>device map for the model.</p> </li> <li> <code>instruction</code>               (<code>Path</code>)           \u2013            <p>path to the instruction file.</p> </li> <li> <code>instruct_in_prompt</code>               (<code>bool</code>)           \u2013            <p>whether to include the instruction in the prompt for models without system role.</p> </li> <li> <code>template</code>               (<code>Path</code>, default:                   <code>None</code> )           \u2013            <p>path to a prompt template file if tokenizer does not include chat template. Optional.</p> </li> <li> <code>lora_path</code>               (<code>Path</code>, default:                   <code>None</code> )           \u2013            <p>path to the LORA adapter.</p> </li> <li> <code>generation</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>generation kwargs.</p> </li> <li> <code>task</code>               (<code>str</code>, default:                   <code>'text-generation'</code> )           \u2013            <p>task for the pipeline.</p> </li> </ul>"},{"location":"reference/models/hf/#ragfoundry.models.hf.HFInference.generate","title":"<code>generate(prompt: str) -&gt; str</code>","text":"<p>Given an input, generate a response.</p>"},{"location":"reference/models/hf/#ragfoundry.models.hf.HFTrain","title":"<code>HFTrain</code>","text":"<p>Class for training HF models locally.</p>"},{"location":"reference/models/hf/#ragfoundry.models.hf.HFTrain.__init__","title":"<code>__init__(model_name_or_path, torch_dtype, device_map, lora: LoraConfig = None, generation=None, completion_start: str = '', instruction_in_prompt=None, max_sequence_len=None, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>model_name_or_path</code>           \u2013            <p>str - HF model name or path.</p> </li> <li> <code>torch_dtype</code>           \u2013            <p>str - torch dtype for the model.</p> </li> <li> <code>device_map</code>           \u2013            <p>dict - device map for the model.</p> </li> <li> <code>lora</code>               (<code>LoraConfig</code>, default:                   <code>None</code> )           \u2013            <p>dict - LoRA adapter config.</p> </li> <li> <code>generation</code>           \u2013            <p>dict - generation kwargs.</p> </li> <li> <code>completion_start</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>str - used to find the start of the completion in the prompt.</p> </li> <li> <code>instruction_in_prompt</code>           \u2013            <p>bool - whether to include the instruction in the prompt for models without system role.</p> </li> </ul>"},{"location":"reference/models/openai_executor/","title":"OpenAI","text":""},{"location":"reference/models/openai_executor/#ragfoundry.models.openai_executor.OpenAIExecutor","title":"<code>OpenAIExecutor</code>","text":"<p>Class representing an interface to the Azure OpenAI API.</p>"},{"location":"reference/models/openai_executor/#ragfoundry.models.openai_executor.OpenAIExecutor.__init__","title":"<code>__init__(azure_endpoint: str, api_key: str = None, api_version: str = '2024-02-15-preview', model: str = 'GPT-4-32k-Bot', chat_parameters: dict = None, delay: int = 1)</code>","text":"<p>Initialize the OpenAIExecutor.</p> <p>Parameters:</p> <ul> <li> <code>azure_endpoint</code>               (<code>str</code>)           \u2013            <p>The Azure endpoint.</p> </li> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The API key, can also read of ENV variable.</p> </li> <li> <code>api_version</code>               (<code>str</code>, default:                   <code>'2024-02-15-preview'</code> )           \u2013            <p>The API version.</p> </li> <li> <code>model</code>               (<code>str</code>, default:                   <code>'GPT-4-32k-Bot'</code> )           \u2013            <p>The model to use, sometimes called deployment or engine.</p> </li> <li> <code>chat_parameters</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>The chat parameters.</p> </li> <li> <code>delay</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>delay between calls.</p> </li> </ul>"},{"location":"reference/models/openai_executor/#ragfoundry.models.openai_executor.OpenAIExecutor.chat","title":"<code>chat(prompt: Union[List, str], instruction: str = None) -&gt; str</code>","text":"<p>Chat with the OpenAI API.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>Union[List, str]</code>)           \u2013            <p>The prompt to chat.</p> </li> <li> <code>instruction</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The instruction to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The response. Empty string if error.</p> </li> </ul>"},{"location":"reference/processing/pipeline/","title":"Pipeline","text":""},{"location":"reference/processing/pipeline/#ragfoundry.processing.pipeline.DataPipeline","title":"<code>DataPipeline</code>","text":"<p>Class for creating a data pipeline.</p> <p>The pipeline holds the list of steps and run them one after the other. The datasets are stored in a global dictionary, where datasets are referred by a key name, as indicated in the <code>inputs</code> parameter for each step. The pipeline manages the cache lookup and creation.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of the pipeline.</p> </li> <li> <code>output_path</code>               (<code>str</code>, default:                   <code>'.'</code> )           \u2013            <p>Path to store the cache files. Defaults to \".\".</p> </li> <li> <code>cache</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to cache the datasets. Defaults to True.</p> </li> <li> <code>steps</code>               (<code>List[BaseStep]</code>, default:                   <code>[]</code> )           \u2013            <p>List of steps in the pipeline. Defaults to [].</p> </li> <li> <code>inputs</code>               (<code>str</code>, default:                   <code>'main_dataset'</code> )           \u2013            <p>Name of the main dataset. Defaults to \"main_dataset\".</p> </li> </ul>"},{"location":"reference/processing/pipeline/#ragfoundry.processing.pipeline.DataPipeline.cache_step","title":"<code>cache_step(step, step_index)</code>","text":"<p>Write to cache-files the current state of the global datasets dictionary for the given inputs.</p>"},{"location":"reference/processing/pipeline/#ragfoundry.processing.pipeline.DataPipeline.delete_cache","title":"<code>delete_cache()</code>","text":"<p>Removing cache files for all steps, cleaning the pipeline.</p>"},{"location":"reference/processing/pipeline/#ragfoundry.processing.pipeline.DataPipeline.gen_cache_fn","title":"<code>gen_cache_fn(step, index, dataset_name)</code>","text":"<p>Create a unique cache filename for  a given dataset, at a given step, in a given index. Uses the step name, inputs, hash and pipeline's path and name and dataset name.</p> <p>Returns a string.</p>"},{"location":"reference/processing/pipeline/#ragfoundry.processing.pipeline.DataPipeline.get_cache_mapping","title":"<code>get_cache_mapping(step: BaseStep, index: int)</code>","text":"<p>Returns a mapping between input datasets and cache filenames, for a given step.</p>"},{"location":"reference/processing/pipeline/#ragfoundry.processing.pipeline.DataPipeline.load_from_cache","title":"<code>load_from_cache(caches_map)</code>","text":"<p>Load datasets from cache using a cache_map. Updates the global datasets dictionary.</p> <p>Internal function, shouldn't be used by the user.</p>"},{"location":"reference/processing/pipeline/#ragfoundry.processing.pipeline.DataPipeline.process","title":"<code>process()</code>","text":"<p>Run pipeline, step after step.</p> <p>Caching is handled here. A step is calculated either if there was a change in the pipeline at a previous step OR the current step has no cache file.</p> <p>When a step is calculated, it is cached and self.last_update is updated to the current step index.</p>"},{"location":"reference/processing/step/","title":"Step","text":""},{"location":"reference/processing/step/#ragfoundry.processing.step.BaseStep","title":"<code>BaseStep</code>","text":"<p>Class representing a step in a processing pipeline. Step can be cached to prevent re-computation. Entry point is <code>__call__</code>. Users would inherit either LocalStep or GlobalStep.</p>"},{"location":"reference/processing/step/#ragfoundry.processing.step.BaseStep.__call__","title":"<code>__call__(datasets, **kwargs)</code>","text":"<p>Pipeline is running these steps using <code>__call__</code>.</p>"},{"location":"reference/processing/step/#ragfoundry.processing.step.BaseStep.calc_hash","title":"<code>calc_hash()</code>","text":"<p>Calculate hash for a step based on its properties. Updates the <code>step_hash</code> property.</p>"},{"location":"reference/processing/step/#ragfoundry.processing.step.BaseStep.get_hash","title":"<code>get_hash()</code>","text":"<p>Step hash getter. If hash is not calculated, it calculates it first.</p>"},{"location":"reference/processing/step/#ragfoundry.processing.step.BaseStep.process","title":"<code>process(dataset_name, datasets, **kwargs)</code>","text":"<p>General processing of <code>dataset_name</code> in <code>datasets</code>, in place.</p>"},{"location":"reference/processing/step/#ragfoundry.processing.step.GlobalStep","title":"<code>GlobalStep</code>","text":"<p>               Bases: <code>BaseStep</code></p> <p>Class representing a step in a processing pipeline, processing the entire dataset.</p> <p>The function to overwrite is <code>process_all</code>; the function accepts the dataset and all the other datasets, if needed.</p>"},{"location":"reference/processing/step/#ragfoundry.processing.step.LocalStep","title":"<code>LocalStep</code>","text":"<p>               Bases: <code>BaseStep</code></p> <p>Class representing a step in a processing pipeline, processing individual examples.</p> <p>The function to overwrite is <code>process_item</code>; the function accepts an item, index, and all the other datasets, if needed.</p>"},{"location":"reference/processing/utils/","title":"Utils","text":""},{"location":"reference/processing/utils/#ragfoundry.processing.utils.dict_hash","title":"<code>dict_hash(dictionary: Dict[str, Any]) -&gt; str</code>","text":"<p>Hash dictionary using MD5. Used in step caching; steps are cached based on the signature.</p>"},{"location":"reference/processing/utils/#ragfoundry.processing.utils.is_jsonable","title":"<code>is_jsonable(x)</code>","text":"<p>Test if input is JSON-serializable.</p>"},{"location":"reference/processing/answer_processors/regex/","title":"regex","text":""},{"location":"reference/processing/answer_processors/regex/#ragfoundry.processing.answer_processors.regex.RegexAnswer","title":"<code>RegexAnswer</code>","text":"<p>Extract answers from the text using regular expressions.</p> <p>Pattern is the regular expression used to extract the answer. Stopping pattern is a string used to split the answer.</p> <p>Example: <code>r = RegexAnswer(\"&lt;ANSWER&gt;: (.*)\", \"[,.;]\")</code></p>"},{"location":"reference/processing/answer_processors/regex/#ragfoundry.processing.answer_processors.regex.RegexAnswer.__call__","title":"<code>__call__(text: str)</code>","text":"<p>Extract the answer from the text.</p>"},{"location":"reference/processing/dataset_loaders/loaders/","title":"loaders","text":""},{"location":"reference/processing/dataset_loaders/loaders/#ragfoundry.processing.dataset_loaders.loaders.HFLoader","title":"<code>HFLoader</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Class to load a dataset using the Hugging Face datasets library. Can use either a HuggingFace tag or a local file.</p>"},{"location":"reference/processing/dataset_loaders/loaders/#ragfoundry.processing.dataset_loaders.loaders.LocalLoader","title":"<code>LocalLoader</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Class to load a local dataset, in a JSON format.</p>"},{"location":"reference/processing/global_steps/output/","title":"Output","text":""},{"location":"reference/processing/global_steps/output/#ragfoundry.processing.global_steps.output.OutputData","title":"<code>OutputData</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Simple class to output the dataset to a jsonl file.</p>"},{"location":"reference/processing/global_steps/output/#ragfoundry.processing.global_steps.output.OutputData.__init__","title":"<code>__init__(prefix, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>prefix</code>               (<code>str</code>)           \u2013            <p>Prefix for the output.</p> </li> </ul> <p>The output name is <code>{prefix}-{dataset_keyname}.jsonl</code>.</p>"},{"location":"reference/processing/global_steps/sampling/","title":"Sampling and Fewshot","text":""},{"location":"reference/processing/global_steps/sampling/#ragfoundry.processing.global_steps.sampling.FewShot","title":"<code>FewShot</code>","text":"<p>               Bases: <code>Sampler</code></p> <p>Class to collect fewshot examples from the same or another dataset.</p>"},{"location":"reference/processing/global_steps/sampling/#ragfoundry.processing.global_steps.sampling.FewShot.__init__","title":"<code>__init__(k, output_key='fewshot', input_dataset=None, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Number of examples to collect.</p> </li> <li> <code>output_key</code>               (<code>str</code>, default:                   <code>'fewshot'</code> )           \u2013            <p>output key to use for the collected examples.</p> </li> <li> <code>input_dataset</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the dataset to take the examples from. To use the same dataset, use None.</p> </li> </ul>"},{"location":"reference/processing/global_steps/sampling/#ragfoundry.processing.global_steps.sampling.Sampler","title":"<code>Sampler</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Class to augment a dataset with sampled examples from the same or another dataset.</p> <p>Full examples can be collected, as well as an individual example keys like <code>query</code>, <code>documents</code>, etc.</p> <p>The step can be used to collect negative documents, negative queries and collect fewshot examples. For fewshot examples, use the dedicated <code>FewShot</code> class.</p>"},{"location":"reference/processing/global_steps/sampling/#ragfoundry.processing.global_steps.sampling.Sampler.__init__","title":"<code>__init__(k, input_key=None, output_key='fewshot', input_dataset=None, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Number of examples to collect.</p> </li> <li> <code>input_key</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>a key to collect from the collected examples, or None to take entire example.</p> </li> <li> <code>output_key</code>               (<code>str</code>, default:                   <code>'fewshot'</code> )           \u2013            <p>output key to use for the examples.</p> </li> <li> <code>input_dataset</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the dataset to take the examples from. To use the same dataset, use None.</p> </li> </ul>"},{"location":"reference/processing/global_steps/sampling/#ragfoundry.processing.global_steps.sampling.ShuffleSelect","title":"<code>ShuffleSelect</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Class to optionally shuffle and select a subset of the dataset.</p> <p>Based on the <code>shuffle</code> and <code>select</code> methods of HF Dataset.</p>"},{"location":"reference/processing/global_steps/sampling/#ragfoundry.processing.global_steps.sampling.ShuffleSelect.__init__","title":"<code>__init__(shuffle=None, limit=None, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>shuffle</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Seed for shuffling the dataset.</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of items to select from the dataset.</p> </li> </ul>"},{"location":"reference/processing/local_steps/common_datasets/","title":"Common Datasets","text":""},{"location":"reference/processing/local_steps/common_datasets/#ragfoundry.processing.local_steps.common_datasets.ARCC","title":"<code>ARCC</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Prepare dataset for RAG augmentation.</p>"},{"location":"reference/processing/local_steps/common_datasets/#ragfoundry.processing.local_steps.common_datasets.ASQA","title":"<code>ASQA</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Normalizes ASQA dataset.</p> <p>It has long answer, to be measured with ROUGE-L and multiple short answers, to be measured with string-EM. Long answer is saved in the <code>answers</code> field, while the short answers (list of lists) are saved in the <code>answer-short</code> field.</p>"},{"location":"reference/processing/local_steps/common_datasets/#ragfoundry.processing.local_steps.common_datasets.HotPot","title":"<code>HotPot</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Normalizes NotPotQA dataset to look like NQ, TQA</p>"},{"location":"reference/processing/local_steps/common_datasets/#ragfoundry.processing.local_steps.common_datasets.PubMed","title":"<code>PubMed</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Prepare dataset for RAG augmentation.</p>"},{"location":"reference/processing/local_steps/context/","title":"Context","text":""},{"location":"reference/processing/local_steps/context/#ragfoundry.processing.local_steps.context.ContextHandler","title":"<code>ContextHandler</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Example class for processing retrieved documents.</p> <p>In this simple example, the text is combined with the title.</p>"},{"location":"reference/processing/local_steps/context/#ragfoundry.processing.local_steps.context.ContextHandler.__init__","title":"<code>__init__(docs_key, title_key='title', text_key='content', **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>docs_key</code>               (<code>str</code>)           \u2013            <p>Key to the documents in the item.</p> </li> <li> <code>title_key</code>               (<code>str</code>, default:                   <code>'title'</code> )           \u2013            <p>Key to the title in the document.</p> </li> <li> <code>text_key</code>               (<code>str</code>, default:                   <code>'content'</code> )           \u2013            <p>Key to the text in the document.</p> </li> </ul>"},{"location":"reference/processing/local_steps/context/#ragfoundry.processing.local_steps.context.DocumentsJoiner","title":"<code>DocumentsJoiner</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Class to select top-K and join the documents into a string.</p>"},{"location":"reference/processing/local_steps/context/#ragfoundry.processing.local_steps.context.DocumentsJoiner.__init__","title":"<code>__init__(docs_key, k=None, join_string='\\n', **kwargs)</code>","text":"<pre><code>    Args:\n        docs_key (str): Key to the documents in the item.\n        k (int, optional): Number of documents to select or take all. Defaults to None.\n        join_string (str): String to join the documents. Defaults to \"\n</code></pre> <p>\".</p>"},{"location":"reference/processing/local_steps/formatting/","title":"Formatting","text":""},{"location":"reference/processing/local_steps/formatting/#ragfoundry.processing.local_steps.formatting.ColumnUpdater","title":"<code>ColumnUpdater</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Simple class to create new columns from existing columns in a dataset. Existing columns are not modified.</p> <p>Parameters:</p> <ul> <li> <code>keys_mapping</code>               (<code>dict</code>)           \u2013            <p>Dictionary with \"from:to\" mapping.</p> </li> </ul>"},{"location":"reference/processing/local_steps/formatting/#ragfoundry.processing.local_steps.formatting.FlattenList","title":"<code>FlattenList</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Class to join a list of strings into a single string.</p>"},{"location":"reference/processing/local_steps/formatting/#ragfoundry.processing.local_steps.formatting.FlattenList.__init__","title":"<code>__init__(input_key, output_key, string_join=', ', **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>input_key</code>               (<code>str</code>)           \u2013            <p>Key to the list of strings.</p> </li> <li> <code>output_key</code>               (<code>str</code>)           \u2013            <p>Key to store the joined string.</p> </li> <li> <code>string_join</code>               (<code>str</code>, default:                   <code>', '</code> )           \u2013            <p>String to join the list of strings. Defaults to \", \".</p> </li> </ul>"},{"location":"reference/processing/local_steps/prompter/","title":"Prompt Creation","text":""},{"location":"reference/processing/local_steps/prompter/#ragfoundry.processing.local_steps.prompter.FewshotPrompter","title":"<code>FewshotPrompter</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Class for formatting fewshot examples into a string, to be used in a prompt.</p> <p>The prompt template contains a placeholder for the fewshot examples; this class is used to format the examples into a string.</p>"},{"location":"reference/processing/local_steps/prompter/#ragfoundry.processing.local_steps.prompter.FewshotPrompter.__init__","title":"<code>__init__(prompt_file: str, fewshot_key: str, mapping: dict, output_key: str, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>prompt_file</code>               (<code>str</code>)           \u2013            <p>Path to the prompt file for the individual fewshot examples.</p> </li> <li> <code>fewshot_key</code>               (<code>str</code>)           \u2013            <p>Key to the fewshot examples in the item.</p> </li> <li> <code>mapping</code>               (<code>dict</code>)           \u2013            <p>Mapping of the placeholders in the prompt to the item keys.</p> </li> <li> <code>output_key</code>               (<code>str</code>)           \u2013            <p>Key to store the formatted fewshot examples.</p> </li> </ul>"},{"location":"reference/processing/local_steps/prompter/#ragfoundry.processing.local_steps.prompter.TextPrompter","title":"<code>TextPrompter</code>","text":"<p>               Bases: <code>LocalStep</code></p> <pre><code>Class for creating prompts. The input is a prompt file with placeholders, a mapping of the placeholders to the item keys, and the key to store the result.\n\nExample: the prompt file is \"prompt.txt\" with the content \"{query}?\n</code></pre> <p>{answer}\".     The mapping is {\"query\": \"question\", \"answer\": \"solution\"}.     The result key is \"prompt\".</p>"},{"location":"reference/processing/local_steps/raft/","title":"RAFT","text":""},{"location":"reference/processing/local_steps/raft/#ragfoundry.processing.local_steps.raft.RAFTStep","title":"<code>RAFTStep</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Implementation of RAFT: Adapting Language Model to Domain Specific RAG.</p> <p>This class compiles a list of negative documents with probability <code>raft_p</code>, and a combination of positive and negative documents with probability 1 - <code>raft_p</code>.</p> <p>Zhang, Tianjun, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E. Gonzalez. 2024. \u201cRAFT: Adapting Language Model to Domain Specific RAG.\u201d arXiv. http://arxiv.org/abs/2403.10131.</p>"},{"location":"reference/processing/local_steps/raft/#ragfoundry.processing.local_steps.raft.RAFTStep.__init__","title":"<code>__init__(k: int = 5, raft_p=0.5, neg_docs_num=5, positive_key='positive_passages', negative_key='negative_passages', output_key='docs', **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>k</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>The number of positive passages to consider.</p> </li> <li> <code>raft_p</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The probability of using positive passages. Defaults to 0.5.</p> </li> <li> <code>neg_docs_num</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>The number of negative passages to consider. Defaults to 2.</p> </li> <li> <code>positive_key</code>               (<code>str</code>, default:                   <code>'positive_passages'</code> )           \u2013            <p>The key containing the positive passages. Defaults to \"positive_passages\".</p> </li> <li> <code>negative_key</code>               (<code>str</code>, default:                   <code>'negative_passages'</code> )           \u2013            <p>The key containing the negative passages. Defaults to \"negative_passages\".</p> </li> <li> <code>output_key</code>               (<code>str</code>, default:                   <code>'docs'</code> )           \u2013            <p>The key to store the output. Defaults to \"docs\".</p> </li> </ul>"},{"location":"reference/processing/local_steps/api/openai/","title":"OpenAI Chat","text":""},{"location":"reference/processing/local_steps/api/openai/#ragfoundry.processing.local_steps.api.openai.OpenAIChat","title":"<code>OpenAIChat</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Interaction with OpenAI service.</p> <p>Model is represented by the <code>OpenAIExecutor</code>.</p> <p>This step is a wrapper, extracting the prompt from the item, interact with the API, and saves the response to the <code>answer</code> key in the item.</p>"},{"location":"reference/processing/local_steps/retrievers/haystack/","title":"Haystack","text":""},{"location":"reference/processing/local_steps/retrievers/haystack/#ragfoundry.processing.local_steps.retrievers.haystack.HaystackRetriever","title":"<code>HaystackRetriever</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Class for document retrieval using Haystack v2 pipelines.</p>"},{"location":"reference/processing/local_steps/retrievers/haystack/#ragfoundry.processing.local_steps.retrievers.haystack.HaystackRetriever.default_query_function","title":"<code>default_query_function(query)</code>","text":"<p>Create the default querying of the pipeline, by inserting the input query into all mandatory fields.</p>"},{"location":"reference/processing/local_steps/retrievers/haystack/#ragfoundry.processing.local_steps.retrievers.haystack.HaystackRetriever.process_item","title":"<code>process_item(item, index, datasets, **kwargs)</code>","text":"<p>Query the <code>query_key</code> in the item and store the results in the <code>docs_key</code>. Retrieved documents are stored as a list of dictionaries with keys <code>content</code> and <code>title</code>.</p>"},{"location":"reference/processing/local_steps/retrievers/haystack/#ragfoundry.processing.local_steps.retrievers.haystack.HaystackRetriever.query","title":"<code>query(query, structure=None)</code>","text":"<p>Haystack v2 pipelines can have multiple inputs; structure specify how to call <code>pipe.run</code>.</p> <p>For example, structure could look like this: {     \"Retriever\": {\"query\": \"query\",},     \"Reranker\": {\"query\": \"query\"}, } and we replace the value of each key with the query.</p>"}]}