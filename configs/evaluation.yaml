answer_processor:
  _target_: ragfoundry.processing.answer_processors.regex.RegexAnswer
  capture_pattern:          # "<ANSWER>: (.*)"
  stopping_pattern:         # "[,.;]"

metrics:
  - _target_: ragfoundry.evaluation.metrics.HFEvaluate
    metric_names: [rouge]
  - _target_: ragfoundry.evaluation.metrics.EM
  - _target_: ragfoundry.evaluation.metrics.StringEM
  - _target_: ragfoundry.evaluation.metrics.F1
  - _target_: ragfoundry.evaluation.metrics.BERTScore
    model: microsoft/deberta-large-mnli
  - _target_: ragfoundry.evaluation.metrics.Semantic
    model: vectara/hallucination_evaluation_model
  - _target_: src.evaluation.metrics.Classification
    mapping: {"yes": 1, "no": 0, "maybe": 2}
    else_value: 2
  - _target_: ragfoundry.evaluation.deep.Faithfulness
    azure_endpoint:
    azure_deployment:
    api_version:
  - _target_: ragfoundry.evaluation.deep.Relevancy
    azure_endpoint:
    azure_deployment:
    api_version:
    embeddings: BAAI/bge-small-en-v1.5


key_names:
  generated: generated
  label: answer
  query: query
  context: context

results_file: my-evaluation.yaml
generated_file: inference.jsonl
data_file: my-processed-data.jsonl
limit:
use_wandb:
experiment:
wandb_entity: